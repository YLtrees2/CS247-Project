{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CS247 Anime Recommendation Engine\n\nThis notebook serves as project for CS247 Advanced Data Mining.\n\nGroup member:\n* Yiming Shi 905525611\n* Penghai Wei 105726519\n* Yongqian Li 004997466\n* Yanxun Li 005712570\n\nIn this notebook we will build a recomender system using dataset **Anime Recommendation Database 2020**.\n\nThe notebook contains the following serveral sections:\n* Input Processing\n* Baseline Construction\n* Side Feature Embedding\n* Utilizing Side Feature","metadata":{}},{"cell_type":"markdown","source":"# Input Processing\n\nIn this section, we mainly extract the basic input from \"animelist.csv\". We cleaned the data, remove duplicate data, normalized the score value, and present an overview of the rating matrix.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:03:20.211333Z","iopub.execute_input":"2022-03-02T18:03:20.211661Z","iopub.status.idle":"2022-03-02T18:03:20.233662Z","shell.execute_reply.started":"2022-03-02T18:03:20.211573Z","shell.execute_reply":"2022-03-02T18:03:20.233014Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Loading input\nINPUT_DIR = '/kaggle/input/anime-recommendation-database-2020'\n!ls {INPUT_DIR}","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:03:20.234998Z","iopub.execute_input":"2022-03-02T18:03:20.235245Z","iopub.status.idle":"2022-03-02T18:03:21.030462Z","shell.execute_reply.started":"2022-03-02T18:03:20.235216Z","shell.execute_reply":"2022-03-02T18:03:21.029511Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Loading Rating Data\nrating_df = pd.read_csv(INPUT_DIR + '/animelist.csv',\n                        nrows=5000000,\n                        low_memory=False, \n                        usecols=[\"user_id\", \"anime_id\", \"rating\"]\n                        )\n\n# User should rate atleast 400 animies\nn_ratings = rating_df['user_id'].value_counts()\nrating_df = rating_df[rating_df['user_id'].isin(n_ratings[n_ratings >= 400].index)].copy()\nlen(rating_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:03:21.032458Z","iopub.execute_input":"2022-03-02T18:03:21.034148Z","iopub.status.idle":"2022-03-02T18:03:23.348863Z","shell.execute_reply.started":"2022-03-02T18:03:21.034098Z","shell.execute_reply":"2022-03-02T18:03:23.348061Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Scaling BTW (0 , 1.0)\nmin_rating = min(rating_df['rating'])\nmax_rating = max(rating_df['rating'])\nrating_df['rating'] = rating_df[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values.astype(np.float64)\n\nAvgRating = np.mean(rating_df['rating'])\nprint('Avg', AvgRating)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:03:23.350546Z","iopub.execute_input":"2022-03-02T18:03:23.350758Z","iopub.status.idle":"2022-03-02T18:03:24.891663Z","shell.execute_reply.started":"2022-03-02T18:03:23.350731Z","shell.execute_reply":"2022-03-02T18:03:24.890799Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Removing Duplicated Rows\nduplicates = rating_df.duplicated()\n\nif duplicates.sum() > 0:\n    print('> {} duplicates'.format(duplicates.sum()))\n    rating_df = rating_df[~duplicates]\n\nprint('> {} duplicates'.format(rating_df.duplicated().sum()))","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:03:24.893132Z","iopub.execute_input":"2022-03-02T18:03:24.893586Z","iopub.status.idle":"2022-03-02T18:03:26.301504Z","shell.execute_reply.started":"2022-03-02T18:03:24.893544Z","shell.execute_reply":"2022-03-02T18:03:26.300539Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Quick review of our rating matrix\ng = rating_df.groupby('user_id')['rating'].count()\ntop_users = g.dropna().sort_values(ascending=False)[:20]\ntop_r = rating_df.join(top_users, rsuffix='_r', how='inner', on='user_id')\n\ng = rating_df.groupby('anime_id')['rating'].count()\ntop_animes = g.dropna().sort_values(ascending=False)[:20]\ntop_r = top_r.join(top_animes, rsuffix='_r', how='inner', on='anime_id')\n\npd.crosstab(top_r.user_id, top_r.anime_id, top_r.rating, aggfunc=np.sum)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:03:26.302566Z","iopub.execute_input":"2022-03-02T18:03:26.302761Z","iopub.status.idle":"2022-03-02T18:03:26.660408Z","shell.execute_reply.started":"2022-03-02T18:03:26.302736Z","shell.execute_reply":"2022-03-02T18:03:26.659366Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Generate Training and testing data.\n\n# Encoding categorical data\nuser_ids = rating_df[\"user_id\"].unique().tolist()\nuser2user_encoded = {x: i for i, x in enumerate(user_ids)}\nuser_encoded2user = {i: x for i, x in enumerate(user_ids)}\nrating_df[\"user\"] = rating_df[\"user_id\"].map(user2user_encoded)\nn_users = len(user2user_encoded)\n\nanime_ids = rating_df[\"anime_id\"].unique().tolist()\nanime2anime_encoded = {x: i for i, x in enumerate(anime_ids)}\nanime_encoded2anime = {i: x for i, x in enumerate(anime_ids)}\nrating_df[\"anime\"] = rating_df[\"anime_id\"].map(anime2anime_encoded)\nn_animes = len(anime2anime_encoded)\n\nprint(\"Num of users: {}, Num of animes: {}\".format(n_users, n_animes))\nprint(\"Min rating: {}, Max rating: {}\".format(min(rating_df['rating']), max(rating_df['rating'])))","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:03:26.662357Z","iopub.execute_input":"2022-03-02T18:03:26.662669Z","iopub.status.idle":"2022-03-02T18:03:27.392278Z","shell.execute_reply.started":"2022-03-02T18:03:26.662629Z","shell.execute_reply":"2022-03-02T18:03:27.391258Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Shuffle\nrating_df = rating_df.sample(frac=1, random_state=73)\n\nX = rating_df[['user', 'anime']].values\ny = rating_df[\"rating\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:03:27.393572Z","iopub.execute_input":"2022-03-02T18:03:27.393795Z","iopub.status.idle":"2022-03-02T18:03:27.976832Z","shell.execute_reply.started":"2022-03-02T18:03:27.393765Z","shell.execute_reply":"2022-03-02T18:03:27.975783Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Split\ntest_set_size = 1000 #1k for test set\ntrain_indices = rating_df.shape[0] - test_set_size \ntest_indicies = rating_df.shape[0] - test_set_size \n\nX_train, X_test, y_train, y_test = (\n    X[:train_indices],\n    X[test_indicies:],\n    y[:train_indices],\n    y[test_indicies:],\n)\n\nprint('> Train set ratings: {}'.format(len(y_train)))\nprint('> Test set ratings: {}'.format(len(y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:03:27.978353Z","iopub.execute_input":"2022-03-02T18:03:27.978737Z","iopub.status.idle":"2022-03-02T18:03:27.986002Z","shell.execute_reply.started":"2022-03-02T18:03:27.978690Z","shell.execute_reply":"2022-03-02T18:03:27.984838Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Baseline Construction\n\nIn this baseline, we use the naive collaborative filtering method. The general idea behind this is that we are predicting what users will like based on their similarity to other users. We simply use the rating matrix generated in the last section, and perform matrix factorization on it.\n","metadata":{}},{"cell_type":"code","source":"# set the training and testing array\nX_train_array = [X_train[:, 0], X_train[:, 1]]\nX_test_array = [X_test[:, 0], X_test[:, 1]]","metadata":{"execution":{"iopub.status.busy":"2022-03-02T09:12:27.359769Z","iopub.execute_input":"2022-03-02T09:12:27.359989Z","iopub.status.idle":"2022-03-02T09:12:27.370450Z","shell.execute_reply.started":"2022-03-02T09:12:27.359944Z","shell.execute_reply":"2022-03-02T09:12:27.369533Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Setup TPU\nimport tensorflow as tf\n\nTPU_INIT = False\n\nif TPU_INIT:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    !nvidia-smi\n    \nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:03:27.988193Z","iopub.execute_input":"2022-03-02T18:03:27.988391Z","iopub.status.idle":"2022-03-02T18:03:34.281831Z","shell.execute_reply.started":"2022-03-02T18:03:27.988366Z","shell.execute_reply":"2022-03-02T18:03:34.279563Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Baseline Model","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras import layers \nimport tensorflow as tf\nfrom keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import Add, Activation, Lambda, BatchNormalization, Concatenate, Dropout, Input, Embedding, Dot, Reshape, Dense, Flatten\n\ndef BaselineNet():\n    embedding_size = 128\n    \n    user = Input(name = 'user', shape = [1])\n    user_embedding = Embedding(name = 'user_embedding',\n                       input_dim = n_users, \n                       output_dim = embedding_size)(user)\n    \n    anime = Input(name = 'anime', shape = [1])\n    anime_embedding = Embedding(name = 'anime_embedding',\n                       input_dim = n_animes, \n                       output_dim = embedding_size)(anime)\n    \n    x = Dot(name = 'dot_product', normalize = True, axes = 2)([user_embedding, anime_embedding])\n    x = Flatten()(x)\n    x = Dense(1, kernel_initializer='he_normal')(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"sigmoid\")(x)\n    \n    model = Model(inputs=[user, anime], outputs=x)\n    model.compile(loss='binary_crossentropy', metrics=[\"mae\", \"mse\"], optimizer='Adam')\n    \n    return model\n\nif TPU_INIT:    \n    with tpu_strategy.scope():\n        model = BaselineNet()\nelse:\n    model = BaselineNet()\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T09:12:28.123056Z","iopub.execute_input":"2022-03-02T09:12:28.123299Z","iopub.status.idle":"2022-03-02T09:12:28.221410Z","shell.execute_reply.started":"2022-03-02T09:12:28.123270Z","shell.execute_reply":"2022-03-02T09:12:28.220607Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Callbacks\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau\n\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005\nbatch_size = 10000\n\nif TPU_INIT:\n    max_lr = max_lr * tpu_strategy.num_replicas_in_sync\n    batch_size = batch_size * tpu_strategy.num_replicas_in_sync\n\nrampup_epochs = 5\nsustain_epochs = 0\nexp_decay = .8\n\ndef lrfn(epoch):\n    if epoch < rampup_epochs:\n        return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n    elif epoch < rampup_epochs + sustain_epochs:\n        return max_lr\n    else:\n        return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n\n\nlr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=0)\n\ncheckpoint_filepath = './weights.h5'\n\nmodel_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,\n                                        save_weights_only=True,\n                                        monitor='val_loss',\n                                        mode='min',\n                                        save_best_only=True)\n\nearly_stopping = EarlyStopping(patience = 3, monitor='val_loss', \n                               mode='min', restore_best_weights=True)\n\nmy_callbacks = [\n    model_checkpoints,\n    lr_callback,\n    #early_stopping,   \n]","metadata":{"execution":{"iopub.status.busy":"2022-03-02T09:12:28.222913Z","iopub.execute_input":"2022-03-02T09:12:28.223298Z","iopub.status.idle":"2022-03-02T09:12:28.233774Z","shell.execute_reply.started":"2022-03-02T09:12:28.223247Z","shell.execute_reply":"2022-03-02T09:12:28.232855Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Model training\nhistory = model.fit(\n    x=X_train_array,\n    y=y_train,\n    batch_size=batch_size,\n    epochs=20,\n    verbose=1,\n    validation_data=(X_test_array, y_test),\n    callbacks=my_callbacks\n)\n\nmodel.load_weights(checkpoint_filepath)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T09:12:28.235297Z","iopub.execute_input":"2022-03-02T09:12:28.235582Z","iopub.status.idle":"2022-03-02T09:16:51.386731Z","shell.execute_reply.started":"2022-03-02T09:12:28.235544Z","shell.execute_reply":"2022-03-02T09:16:51.385877Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#Training results\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(history.history[\"loss\"][0:-2])\nplt.plot(history.history[\"val_loss\"][0:-2])\nplt.title(\"model loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"test\"], loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T09:16:51.388584Z","iopub.execute_input":"2022-03-02T09:16:51.388841Z","iopub.status.idle":"2022-03-02T09:16:51.644482Z","shell.execute_reply.started":"2022-03-02T09:16:51.388811Z","shell.execute_reply":"2022-03-02T09:16:51.643835Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Side Feature Embedding\n\nNoticed in our dataset, we also have the anime name, synopsis, and genres information. We want to use this side features, since it contains many info that might be useful. We utilize a pretrained bert model for embedding and use tensorboard for embeding visualization.","metadata":{}},{"cell_type":"markdown","source":"## Loading Synopsis Data","metadata":{}},{"cell_type":"code","source":"# loading Synopsis Data\nanime_synopsis_df = pd.read_csv(INPUT_DIR + '/anime_with_synopsis.csv', \n                        low_memory=False, \n                        )\nlen(anime_synopsis_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T08:44:09.820075Z","iopub.execute_input":"2022-03-02T08:44:09.820817Z","iopub.status.idle":"2022-03-02T08:44:10.042978Z","shell.execute_reply.started":"2022-03-02T08:44:09.820782Z","shell.execute_reply":"2022-03-02T08:44:10.042048Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"We tried bert model with different size from HuggingFace. We stored the embeding result in pickle files so that we don't need to rerun this section again.\n\nHere is the links for the model we tried.\n* https://huggingface.co/bert-base-uncased\n* https://huggingface.co/distilbert-base-uncased\n* https://huggingface.co/prajjwal1/bert-tiny","metadata":{}},{"cell_type":"code","source":"# loading pretrained bert\nfrom transformers import AutoTokenizer, AutoModel #for embeddings\n\ntokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\",)\nbert_model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\",output_hidden_states=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T01:29:21.657322Z","iopub.execute_input":"2022-03-02T01:29:21.657974Z","iopub.status.idle":"2022-03-02T01:29:29.589149Z","shell.execute_reply.started":"2022-03-02T01:29:21.657933Z","shell.execute_reply":"2022-03-02T01:29:29.588444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create embeddings\ndef get_embeddings(text,token_length):\n    tokens=tokenizer(text,max_length=token_length,padding='max_length',truncation=True)\n    output=bert_model(torch.tensor(tokens.input_ids).unsqueeze(0),\n                 attention_mask=torch.tensor(tokens.attention_mask).unsqueeze(0)).hidden_states[-1]\n    return torch.mean(output,axis=1).detach().numpy()\n\nget_embeddings(\"Action\", 1).shape","metadata":{"execution":{"iopub.status.busy":"2022-03-02T01:29:33.341185Z","iopub.execute_input":"2022-03-02T01:29:33.341473Z","iopub.status.idle":"2022-03-02T01:29:33.35319Z","shell.execute_reply.started":"2022-03-02T01:29:33.341438Z","shell.execute_reply":"2022-03-02T01:29:33.352344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# processing genres\n# get all unique genres\nfrom tqdm import tqdm\nimport pickle\n\ngenres_embeddings_map = {}\n\nfor _, j in tqdm(anime_synopsis_df.iterrows(), total=anime_synopsis_df.shape[0]):\n    gList = j[\"Genres\"].split(', ')\n    for g in gList:\n        if g not in genres_embeddings_map:\n            genre_embedding = get_embeddings(g, 1)\n            genres_embeddings_map[g] = genre_embedding\n\nf = open(\"genre_embedding_map_small.pkl\",\"wb\")\npickle.dump(genres_embeddings_map,f)\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T01:30:07.725233Z","iopub.execute_input":"2022-03-02T01:30:07.726013Z","iopub.status.idle":"2022-03-02T01:30:08.649269Z","shell.execute_reply.started":"2022-03-02T01:30:07.725964Z","shell.execute_reply":"2022-03-02T01:30:08.64839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# translate animies into embedding\nfrom tqdm import tqdm\nimport pickle\n\n# check for Nan\nanime_synopsis_df.fillna('', inplace=True)\n\nanime_embedding_map = {}\n\nfor _, j in tqdm(anime_synopsis_df.iterrows(), total=anime_synopsis_df.shape[0]):\n    name_embedding = get_embeddings(j[\"Name\"], 10)    \n    sypnopsis_embedding = get_embeddings(j[\"sypnopsis\"], 50)\n    anime_embedding_map[j[\"MAL_ID\"]] = np.append(name_embedding[0], sypnopsis_embedding[0])\n    \nf = open(\"anime_embedding_map_small.pkl\",\"wb\")\npickle.dump(anime_embedding_map,f)\nf.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization Using Tensorboard\n\nTo visualize the embedding result. I converted the data into tsv file which tensorboard accepts.\n\nWe could use https://projector.tensorflow.org/ to visualize the embedding result.","metadata":{}},{"cell_type":"code","source":"# visualize \nimport csv\n\ng_list=list(genres_embeddings_map.keys())\ng_embedding_list=list(genres_embeddings_map.values())\nembeddings = np.concatenate(g_embedding_list, axis=0)\n\nnp.savetxt('g_embedding.tsv', embeddings, delimiter='\\t')\n\nwith open('g.tsv', 'w') as f_output:\n    tsv_output = csv.writer(f_output, delimiter='\\n')\n    tsv_output.writerow(g_list)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Embedding\n\nSince we stored the embedding in pickle files, we don't need to recalculate the embeddings. We could load from the pickle files.","metadata":{}},{"cell_type":"code","source":"# loading embeding pkl file\nimport pickle\n\nanime_embedding_map = {}\n# with open('../input/embedding-small/anime_embedding_map.pkl', 'rb') as f: # distilbert-base-uncased\nwith open('../input/embedding-small/anime_embedding_map_small.pkl', 'rb') as f: # prajjwal1/bert-tiny\n    anime_embedding_map = pickle.load(f)\n\nprint(len(anime_embedding_map))","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:03:44.822124Z","iopub.execute_input":"2022-03-02T18:03:44.822763Z","iopub.status.idle":"2022-03-02T18:03:45.513849Z","shell.execute_reply.started":"2022-03-02T18:03:44.822711Z","shell.execute_reply":"2022-03-02T18:03:45.512908Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Use Side Feature Embedding","metadata":{}},{"cell_type":"markdown","source":"## Using Anime Embedding\nFirst we modify the training and testing data from using ids to using anime embedding","metadata":{}},{"cell_type":"code","source":"# change anime to anime embedding\nuser_train = np.array(X_train[:, 0])\nanime_train_list = X_train[:, 1].tolist()\n\nanime_train = []\nfor i in anime_train_list:\n    if i in anime_embedding_map:\n        anime_train.append(anime_embedding_map[i])\n    else:\n        anime_train.append(np.random.rand(256))\nanime_train = np.array(anime_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:03:46.905463Z","iopub.execute_input":"2022-03-02T18:03:46.906293Z","iopub.status.idle":"2022-03-02T18:03:59.519575Z","shell.execute_reply.started":"2022-03-02T18:03:46.906242Z","shell.execute_reply":"2022-03-02T18:03:59.518817Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# change anime to anime embedding\nuser_test = np.expand_dims(np.array(X_test[:, 0]), axis = 1)\nanime_test_list = X_test[:, 1].tolist()\n\nanime_test = []\nfor i in anime_test_list:\n    if i in anime_embedding_map:\n        anime_test.append(anime_embedding_map[i])\n    else:\n        anime_test.append(np.random.rand(256))\nanime_test = np.array(anime_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:03:59.520968Z","iopub.execute_input":"2022-03-02T18:03:59.521187Z","iopub.status.idle":"2022-03-02T18:03:59.529695Z","shell.execute_reply.started":"2022-03-02T18:03:59.521161Z","shell.execute_reply":"2022-03-02T18:03:59.528980Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras import layers \nimport tensorflow as tf\nfrom keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import Add, Activation, Lambda, BatchNormalization, Concatenate, Dropout, Input, Embedding, Dot, Reshape, Dense, Flatten\n\n# Embedding layers\ndef Anime_Embedding_Net():\n    embedding_size = 128\n    \n    user = Input(name = 'user', shape = [1])\n    user_embedding = Embedding(name = 'user_embedding',\n                       input_dim = n_users, \n                       output_dim = embedding_size)(user)\n    user_embedding = Flatten()(user_embedding)\n    \n    anime = Input(name = 'anime', shape = [256])\n    anime_embedding = Dense(512, activation='relu')(anime)\n    anime_embedding = Dense(256, activation='relu')(anime_embedding)\n    anime_embedding = Dense(128, activation='softmax')(anime_embedding)\n\n    \n    x = Dot(name = 'dot_product', normalize = True, axes = 1)([user_embedding, anime_embedding])\n    x = Dense(1, kernel_initializer='he_normal')(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"sigmoid\")(x)\n    \n    model = Model(inputs=[user, anime], outputs=x)\n    model.compile(loss='binary_crossentropy', metrics=[\"mae\", \"mse\"], optimizer='Adam')\n    \n    return model\n\nprint(TPU_INIT)\nif TPU_INIT:    \n    with tpu_strategy.scope():\n        model = Anime_Embedding_Net()\nelse:\n    model = Anime_Embedding_Net()\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:04:28.351977Z","iopub.execute_input":"2022-03-02T18:04:28.352265Z","iopub.status.idle":"2022-03-02T18:04:29.652646Z","shell.execute_reply.started":"2022-03-02T18:04:28.352234Z","shell.execute_reply":"2022-03-02T18:04:29.651829Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Callbacks\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau\n\nstart_lr = 0.0001\nmin_lr = 0.00001\nmax_lr = 0.001\nbatch_size = 4096\n\nif TPU_INIT:\n    max_lr = max_lr * tpu_strategy.num_replicas_in_sync\n    batch_size = batch_size * tpu_strategy.num_replicas_in_sync\n\nrampup_epochs = 5\nsustain_epochs = 0\nexp_decay = .8\n\ndef lrfn(epoch):\n    if epoch < rampup_epochs:\n        return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n    elif epoch < rampup_epochs + sustain_epochs:\n        return max_lr\n    else:\n        return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n\n\nlr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=0)\n\ncheckpoint_filepath = './weights.h5'\n\nmodel_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,\n                                        save_weights_only=True,\n                                        monitor='val_loss',\n                                        mode='min',\n                                        save_best_only=True)\n\nearly_stopping = EarlyStopping(patience = 3, monitor='val_loss', \n                               mode='min', restore_best_weights=True)\n\nmy_callbacks = [\n    model_checkpoints,\n    lr_callback,\n    #early_stopping,   \n]","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:04:40.983843Z","iopub.execute_input":"2022-03-02T18:04:40.984317Z","iopub.status.idle":"2022-03-02T18:04:40.993847Z","shell.execute_reply.started":"2022-03-02T18:04:40.984278Z","shell.execute_reply":"2022-03-02T18:04:40.992928Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Model training\nhistory = model.fit(\n    x=[user_train, anime_train],\n    y=y_train,\n    batch_size=batch_size,\n    epochs=20,\n    verbose=1,\n    validation_data=((user_test, anime_test), y_test),\n    callbacks=my_callbacks\n)\n\nmodel.load_weights(checkpoint_filepath)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:04:44.185742Z","iopub.execute_input":"2022-03-02T18:04:44.186054Z","iopub.status.idle":"2022-03-02T18:24:12.628468Z","shell.execute_reply.started":"2022-03-02T18:04:44.186020Z","shell.execute_reply":"2022-03-02T18:24:12.627682Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Training results\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(history.history[\"loss\"][0:-2])\nplt.plot(history.history[\"val_loss\"][0:-2])\nplt.title(\"model loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"test\"], loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:24:12.630002Z","iopub.execute_input":"2022-03-02T18:24:12.630623Z","iopub.status.idle":"2022-03-02T18:24:12.885505Z","shell.execute_reply.started":"2022-03-02T18:24:12.630589Z","shell.execute_reply":"2022-03-02T18:24:12.884828Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Though the model is a little bit overfit. But we could find that the actual loss is smaller than the baseline.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}